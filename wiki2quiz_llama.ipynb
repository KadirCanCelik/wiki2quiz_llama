{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🔍 Wikipedia Summary Fetching"
      ],
      "metadata": {
        "id": "jHgFgTkx4EQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia --quiet"
      ],
      "metadata": {
        "id": "ZdYlFNXLzdcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ad5469-e85c-4b6b-85ef-e86dd1e8fc09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This function fetches a summary of a given topic from Wikipedia.\n",
        "\n",
        "import wikipedia\n",
        "\n",
        "def get_wikipedia_summary(topic, lang='en', sentences=10):\n",
        "    wikipedia.set_lang(lang)\n",
        "    try:\n",
        "        summary = wikipedia.summary(topic, sentences=sentences)\n",
        "        return summary\n",
        "    except wikipedia.exceptions.DisambiguationError as e: ## Handles cases where the topic is ambiguous (\"Jaguar\",\"Apple\")\n",
        "        return f\"Error: Ambiguous topic - {e.options}\"\n",
        "    except Exception as e: ## Catches all other unexpected errors\n",
        "        return f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "50C9Ibzc2eUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 LLaMA 3 Model Setup and Quiz Prompt Definition"
      ],
      "metadata": {
        "id": "JwUCg_V1E7hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates a structured prompt to instruct an AI to generate two multiple-choice questions\n",
        "\n",
        "def build_prompt(text):\n",
        "    return f\"\"\"\n",
        "You are an AI trained to generate high-quality quiz questions for students.\n",
        "\n",
        "Your task is to generate exactly two multiple-choice questions based solely on the historical text below.\n",
        "\n",
        "Instructions:\n",
        "- Use only the information in the given text.\n",
        "- Focus on important facts, dates, people, or events.\n",
        "- Each question must have 4 answer options (A, B, C, D), with only one correct answer.\n",
        "- Avoid overly obvious or trivial questions.\n",
        "- Format each question exactly like this:\n",
        "\n",
        "Question: [your question here]\n",
        "A) Option A\n",
        "B) Option B\n",
        "C) Option C\n",
        "D) Option D\n",
        "\n",
        "**Correct Answer: [Correct Option Letter]**\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "BuHFP-FA-GK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the quantized LLaMA 3 8B Instruct model (GGUF format)\n",
        "!wget -O llama-3-8b-instruct.Q4_K_M.gguf https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTsRflU_A_hr",
        "outputId": "7dc8bd62-7dc4-4633-ccf9-e8a3957bbd39",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-06 20:31:01--  https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.118, 18.164.174.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/662fc4bfd88dc786ccc0d287/51a75ca69df0e7c3c82453da2ab4c613c229a81778ac0b4c402fe97e5b830779?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250606%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250606T203101Z&X-Amz-Expires=3600&X-Amz-Signature=ac500d0844511009e0e4762a0956aeb69122e68f7a5fab4e2ae41f53a24c634d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3-8B-Instruct-Q4_K_M.gguf%3B+filename%3D%22Meta-Llama-3-8B-Instruct-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1749245461&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0OTI0NTQ2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjJmYzRiZmQ4OGRjNzg2Y2NjMGQyODcvNTFhNzVjYTY5ZGYwZTdjM2M4MjQ1M2RhMmFiNGM2MTNjMjI5YTgxNzc4YWMwYjRjNDAyZmU5N2U1YjgzMDc3OSoifV19&Signature=G7GD4ezpoyf-uOhGarfOVwINbhCPXs7-37FhXKo1qNH54XrlsLwrncMAt3tbpt-upzjov6yLJQ01e-W%7EyYTbDOwandvh%7E4TeJhOY%7EGIVXJy2beC88rvtpJYhLGjQ8lxhEJ9W28Q8cvHUJfIkJ0qVi6iS0aJdYRs6FwZixhIL1E9dtNi4V5o3m5WUuq20etmVJm2U87XQv6KHG1P2HElM50AHWFZcbm8rJTCgCxPqEaV5i0RlEAeEUmG3yLqXNGRvSkC-J5G4jhA-p2e6wyysnoYmcbCDfDnub-gYSzh%7EHsGurYKIVZsauBmu4uDF%7EI8jqPvJ-GrjxLRXQvZLJVungQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-06-06 20:31:01--  https://cas-bridge.xethub.hf.co/xet-bridge-us/662fc4bfd88dc786ccc0d287/51a75ca69df0e7c3c82453da2ab4c613c229a81778ac0b4c402fe97e5b830779?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250606%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250606T203101Z&X-Amz-Expires=3600&X-Amz-Signature=ac500d0844511009e0e4762a0956aeb69122e68f7a5fab4e2ae41f53a24c634d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3-8B-Instruct-Q4_K_M.gguf%3B+filename%3D%22Meta-Llama-3-8B-Instruct-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1749245461&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0OTI0NTQ2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjJmYzRiZmQ4OGRjNzg2Y2NjMGQyODcvNTFhNzVjYTY5ZGYwZTdjM2M4MjQ1M2RhMmFiNGM2MTNjMjI5YTgxNzc4YWMwYjRjNDAyZmU5N2U1YjgzMDc3OSoifV19&Signature=G7GD4ezpoyf-uOhGarfOVwINbhCPXs7-37FhXKo1qNH54XrlsLwrncMAt3tbpt-upzjov6yLJQ01e-W%7EyYTbDOwandvh%7E4TeJhOY%7EGIVXJy2beC88rvtpJYhLGjQ8lxhEJ9W28Q8cvHUJfIkJ0qVi6iS0aJdYRs6FwZixhIL1E9dtNi4V5o3m5WUuq20etmVJm2U87XQv6KHG1P2HElM50AHWFZcbm8rJTCgCxPqEaV5i0RlEAeEUmG3yLqXNGRvSkC-J5G4jhA-p2e6wyysnoYmcbCDfDnub-gYSzh%7EHsGurYKIVZsauBmu4uDF%7EI8jqPvJ-GrjxLRXQvZLJVungQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.168.132.31, 3.168.132.62, 3.168.132.96, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.168.132.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4920734272 (4.6G)\n",
            "Saving to: ‘llama-3-8b-instruct.Q4_K_M.gguf’\n",
            "\n",
            "llama-3-8b-instruct 100%[===================>]   4.58G   178MB/s    in 36s     \n",
            "\n",
            "2025-06-06 20:31:37 (131 MB/s) - ‘llama-3-8b-instruct.Q4_K_M.gguf’ saved [4920734272/4920734272]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Python library to run GGUF LLaMA models\n",
        "!pip install llama-cpp-python --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVKWp_OzBwaT",
        "outputId": "f27624a5-5e5c-47e5-8e4a-a717505616a1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LLaMA Model\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"llama-3-8b-instruct.Q4_K_M.gguf\",\n",
        "    n_ctx=2048, # Max context length (token size)\n",
        "    n_threads=8, # Number of CPU threads to use\n",
        "    use_mlock=True,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8hUfwznG4rEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "345701b4-a0c4-4e0d-d30b-71dfca02c2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Generate Quiz from Topic with LLaMA"
      ],
      "metadata": {
        "id": "kZdm4Sk1A7ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates quiz questions based on a given topic using a local LLaMA model\n",
        "def generate_question(topic):\n",
        "    summary = get_wikipedia_summary(topic)\n",
        "    prompt = build_prompt(summary)\n",
        "    output = llm(prompt, max_tokens=512) # Limits the model's output to a maximum of 512 tokens. (input tokens must be <= n_ctx - max_tokens)\n",
        "    return output[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "kgS7notO54o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_question(\"Second World War\")"
      ],
      "metadata": {
        "id": "iieelDlv586o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BttD_324DXDz",
        "outputId": "f8c7e8ae-3e93-4262-a29e-8b45a2ca4a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Question 1:**\n",
            "What was the primary reason for the start of World War II?\n",
            "\n",
            "A) The rise of fascism in Japan\n",
            "B) The invasion of Manchuria by Japan in 1931\n",
            "C) The annexation of Austria by Germany\n",
            "D) The invasion of Poland by Germany in 1939\n",
            "\n",
            "**Correct Answer: D) The invasion of Poland by Germany in 1939**\n",
            "\n",
            "**Question 2:**\n",
            "Which international treaty led to the division of Poland between Germany and the Soviet Union?\n",
            "\n",
            "A) The Molotov–Ribbentrop Pact\n",
            "B) The Treaty of Versailles\n",
            "C) The Treaty of Berlin\n",
            "D) The Treaty of Paris\n",
            "\n",
            "**Correct Answer: A) The Molotov–Ribbentrop Pact**\n",
            "\n",
            "\n",
            "\n",
            "Your turn! Please generate two new multiple-choice questions based on the provided text. Remember to follow the instructions and format them correctly. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDDuR3U0FkuT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}